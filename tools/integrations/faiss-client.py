"""
Cliente FAISS para Kai
GestiÃ³n de memoria vectorial y bÃºsqueda semÃ¡ntica
"""

from typing import List, Tuple, Optional, Dict, Any
import numpy as np
import pickle
import os

try:
    import faiss
except ImportError:
    faiss = None
    print("âš ï¸ FAISS no estÃ¡ instalado. Ejecuta: tools/setup/install-faiss.sh")


class FAISSMemoryClient:
    """Cliente para gestiÃ³n de memoria vectorial de Kai usando FAISS"""
    
    def __init__(
        self, 
        dimension: int = 768,
        index_type: str = "FlatL2",
        use_gpu: bool = False
    ):
        """
        Inicializa el cliente FAISS
        
        Args:
            dimension: DimensiÃ³n de los vectores de embedding
            index_type: Tipo de Ã­ndice ('FlatL2', 'IVFFlat', 'HNSW')
            use_gpu: Usar GPU si estÃ¡ disponible
        """
        if faiss is None:
            raise ImportError(
                "FAISS no estÃ¡ instalado. "
                "Ejecuta: tools/setup/install-faiss.sh"
            )
        
        self.dimension = dimension
        self.index_type = index_type
        self.use_gpu = use_gpu
        
        # Crear Ã­ndice segÃºn tipo
        if index_type == "FlatL2":
            self.index = faiss.IndexFlatL2(dimension)
        elif index_type == "IVFFlat":
            # IVF requiere entrenamiento
            quantizer = faiss.IndexFlatL2(dimension)
            self.index = faiss.IndexIVFFlat(quantizer, dimension, 100)
        elif index_type == "HNSW":
            # HNSW para bÃºsqueda rÃ¡pida
            self.index = faiss.IndexHNSWFlat(dimension, 32)
        else:
            raise ValueError(f"Tipo de Ã­ndice no soportado: {index_type}")
        
        # Mover a GPU si estÃ¡ disponible
        if use_gpu and faiss.get_num_gpus() > 0:
            self.index = faiss.index_cpu_to_gpu(
                faiss.StandardGpuResources(), 
                0, 
                self.index
            )
            print("ğŸ® Usando GPU para FAISS")
        
        # Metadatos asociados a cada vector
        self.metadata: List[Dict[str, Any]] = []
        
    def add_memory(
        self, 
        embedding: np.ndarray, 
        metadata: Optional[Dict[str, Any]] = None
    ) -> int:
        """
        AÃ±ade un recuerdo al Ã­ndice
        
        Args:
            embedding: Vector de embedding (debe ser dimension correcta)
            metadata: Datos asociados al recuerdo (texto, timestamp, etc.)
            
        Returns:
            ID del recuerdo aÃ±adido
        """
        if embedding.shape[-1] != self.dimension:
            raise ValueError(
                f"DimensiÃ³n incorrecta. Esperado: {self.dimension}, "
                f"Recibido: {embedding.shape[-1]}"
            )
        
        # Asegurar formato correcto
        if embedding.ndim == 1:
            embedding = embedding.reshape(1, -1)
        
        embedding = embedding.astype('float32')
        
        # AÃ±adir al Ã­ndice
        self.index.add(embedding)
        
        # Guardar metadata
        memory_id = len(self.metadata)
        self.metadata.append(metadata or {})
        
        return memory_id
    
    def add_memories_batch(
        self, 
        embeddings: np.ndarray,
        metadata_list: Optional[List[Dict[str, Any]]] = None
    ) -> List[int]:
        """
        AÃ±ade mÃºltiples recuerdos en batch
        
        Args:
            embeddings: Array de embeddings (N x dimension)
            metadata_list: Lista de metadatos para cada embedding
            
        Returns:
            Lista de IDs de recuerdos aÃ±adidos
        """
        embeddings = embeddings.astype('float32')
        
        start_id = len(self.metadata)
        self.index.add(embeddings)
        
        # AÃ±adir metadata
        if metadata_list:
            self.metadata.extend(metadata_list)
        else:
            self.metadata.extend([{}] * len(embeddings))
        
        return list(range(start_id, start_id + len(embeddings)))
    
    def search(
        self, 
        query_embedding: np.ndarray,
        k: int = 5,
        return_distances: bool = True
    ) -> Tuple[List[int], List[float], List[Dict[str, Any]]]:
        """
        Busca los k recuerdos mÃ¡s similares
        
        Args:
            query_embedding: Vector de consulta
            k: NÃºmero de resultados a retornar
            return_distances: Si retornar distancias
            
        Returns:
            Tupla (ids, distancias, metadatos)
        """
        if query_embedding.ndim == 1:
            query_embedding = query_embedding.reshape(1, -1)
        
        query_embedding = query_embedding.astype('float32')
        
        # BÃºsqueda
        distances, indices = self.index.search(query_embedding, k)
        
        # Obtener metadata
        results_metadata = [
            self.metadata[idx] if idx < len(self.metadata) else {}
            for idx in indices[0]
        ]
        
        if return_distances:
            return indices[0].tolist(), distances[0].tolist(), results_metadata
        else:
            return indices[0].tolist(), [], results_metadata
    
    def get_total_memories(self) -> int:
        """Retorna el nÃºmero total de recuerdos almacenados"""
        return self.index.ntotal
    
    def save(self, filepath: str):
        """
        Guarda el Ã­ndice y metadata a disco
        
        Args:
            filepath: Ruta base para guardar archivos
        """
        # Guardar Ã­ndice FAISS
        if self.use_gpu:
            # Mover a CPU antes de guardar
            index_cpu = faiss.index_gpu_to_cpu(self.index)
            faiss.write_index(index_cpu, f"{filepath}.index")
        else:
            faiss.write_index(self.index, f"{filepath}.index")
        
        # Guardar metadata
        with open(f"{filepath}.metadata.pkl", 'wb') as f:
            pickle.dump(self.metadata, f)
        
        print(f"ğŸ’¾ Ãndice guardado en {filepath}")
    
    def load(self, filepath: str):
        """
        Carga el Ã­ndice y metadata desde disco
        
        Args:
            filepath: Ruta base de los archivos
        """
        # Cargar Ã­ndice
        self.index = faiss.read_index(f"{filepath}.index")
        
        # Mover a GPU si es necesario
        if self.use_gpu and faiss.get_num_gpus() > 0:
            self.index = faiss.index_cpu_to_gpu(
                faiss.StandardGpuResources(),
                0,
                self.index
            )
        
        # Cargar metadata
        with open(f"{filepath}.metadata.pkl", 'rb') as f:
            self.metadata = pickle.load(f)
        
        print(f"ğŸ“‚ Ãndice cargado desde {filepath}")
        print(f"ğŸ“Š Total recuerdos: {self.get_total_memories()}")


# Ejemplo de uso
if __name__ == "__main__":
    # Inicializar cliente
    client = FAISSMemoryClient(dimension=768)
    
    # Simular embeddings (en producciÃ³n vendrÃ­an de un modelo como BERT)
    print("ğŸ§  Creando memoria de Kai...")
    
    # AÃ±adir recuerdos
    memories = [
        {"text": "El usuario prefiere jugar D&D los viernes", "type": "preference"},
        {"text": "El nombre del personaje principal es Aragorn", "type": "character"},
        {"text": "La Ãºltima sesiÃ³n fue en la Taberna del DragÃ³n", "type": "location"},
    ]
    
    for memory in memories:
        # Simular embedding (en producciÃ³n usar modelo real)
        embedding = np.random.random(768).astype('float32')
        client.add_memory(embedding, metadata=memory)
    
    print(f"âœ… {client.get_total_memories()} recuerdos aÃ±adidos")
    
    # Buscar recuerdos similares
    print("\nğŸ” Buscando recuerdos relacionados...")
    query = np.random.random(768).astype('float32')
    ids, distances, metadata = client.search(query, k=3)
    
    print("\nğŸ“‹ Recuerdos mÃ¡s relevantes:")
    for i, (id, dist, meta) in enumerate(zip(ids, distances, metadata)):
        print(f"  {i+1}. [distancia: {dist:.4f}] {meta.get('text', 'Sin texto')}")
    
    # Guardar Ã­ndice
    print("\nğŸ’¾ Guardando Ã­ndice...")
    client.save("/tmp/kai_memory")
    
    # Cargar Ã­ndice
    print("\nğŸ“‚ Cargando Ã­ndice...")
    new_client = FAISSMemoryClient(dimension=768)
    new_client.load("/tmp/kai_memory")
